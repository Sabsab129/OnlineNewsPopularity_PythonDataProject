{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "## [First imports](#section1)\n",
    "### [Importing modules](#section11)\n",
    "### [Importing the dataset](#section12)\n",
    "## [Data visualization and preprocessing](#section2)\n",
    "### [Naive introductory overview of the dataset](#section21)\n",
    "### [Formating the dataset](#section22)\n",
    "#### [Dataset's description](#section221)\n",
    "#### [Dataset's columns](#section222)\n",
    "#### [Dataset's missing and misc. values](#section223)\n",
    "#### [Dataset's outliers](#section224)\n",
    "### [Transforming the dataset](#section23)\n",
    "#### [Generic methods for vizualization and transformation](#section230)\n",
    "#### [Examining the target ***shares***](#section231)\n",
    "#### [Transforming the feature ***n_tokens_title***](#section232)\n",
    "#### [Transforming the feature ***n_tokens_content***](#section233)\n",
    "#### [Transforming the feature ***n_non_stop_words***](#section234)\n",
    "#### [Transforming the feature ***num_hrefs***](#section235)\n",
    "#### [Transforming the feature ***num_self_hrefs***](#section236)\n",
    "#### [Transforming the feature ***num_imgs***](#section237)\n",
    "#### [Transforming the feature ***num_videos***](#section238)\n",
    "#### [Transforming the features ***rate_positive_words*** and ***rate_negative_words***](#section239)\n",
    "#### [Transforming the features ***keywords***](#section2310)\n",
    "#### [Transforming the features ***self_reference***](#section2311)\n",
    "#### [Transforming the features ***data_channel***](#section2312)\n",
    "#### [Transforming the features ***weekday*** and ***is_weekend***](#section2313)\n",
    "#### [Transforming the features ***LDA***](#section2314)\n",
    "#### [Transforming the features ***global***](#section2315)\n",
    "#### [Transforming the features ***title***](#section2316)\n",
    "#### [Transforming the features ***polarity***](#section2317)\n",
    "### [Regression models](#section3)\n",
    "### [Flask regression](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First imports <a class = \"anchor\" id = \"section1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules <a class = \"anchor\" id = \"section11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for plotting the dataset\n",
    "import seaborn as sns # heatmap\n",
    "import pandas as pd # import and transform the dataset\n",
    "import numpy as np # calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset <a class = \"anchor\" id = \"section12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"OnlineNewsPopularity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization and preprocessing <a class = \"anchor\" id = \"section2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive introductory overview of the dataset <a class = \"anchor\" id = \"section21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating the dataset <a class = \"anchor\" id = \"section22\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset's description <a class = \"anchor\" id = \"section221\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset's description specifically considers the first two features ***url*** and ***timedelta*** as non-predictive. Let us remove them from the main dataset. Nevertheless, we'll store them elsewhere for a potential alternative model to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = data['url']\n",
    "data_timedelta = data[' timedelta']\n",
    "data = data.drop(['url', ' timedelta'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset's columns <a class = \"anchor\" id = \"section222\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns because they have a space in it. It will be easier to handle further the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.replace('ss', 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset's missing and misc. values <a class = \"anchor\" id = \"section223\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check for any missing or duplicate data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url.nunique() == len(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(data.isna().sum() != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are neither *NA* nor duplicated values. However, when transforming some features, the dataset revealed some values that could be considered *NA* in spirit. For example, let us examine the ***rate_positive_words*** and ***rate_negative_words*** case :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['rate_positive_words',\n",
    "      'rate_negative_words']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the sum of the two columns is always equal to 1. This is no coincidence as the following command showcases the $y = 1 - x$ relationship that the pair of features respect :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data[\"rate_positive_words\"] + data[\"rate_negative_words\"] == 1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a non-negligeable error rate on our presumed relationship. Let us check the indexes that are allegedly to be exceptions to our rule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['rate_positive_words'] + data['rate_negative_words'] != 1][['rate_positive_words', 'rate_negative_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undeniably, $0.827957 + 0.172043 = 1.000000$. So this must be a mistake on either Python's or the source's end. Let's suppose that Python returns a correct result with 1% error :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['rate_positive_words'] + data['rate_negative_words'] < 0.99][['rate_positive_words', 'rate_negative_words']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily deduce that the remaining indexes were not correctly set up by the algorithm that put up the dataset. In case one might still believe in an article that has 0% rate of positive words and negative words, let's check the other features present for these indexes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['rate_positive_words'] + data['rate_negative_words'] < 0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, most of the values are absent. When checking one of the urls, we were fully certain that this was a mistake that needed to be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = data.index[data['rate_positive_words'] + data['rate_negative_words'] == 0.00].tolist()\n",
    "data = data.drop(index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset's outliers <a class = \"anchor\" id = \"section224\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have an overall look at the distribution of the features as well as the target ***shares***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 20,25\n",
    "fig = data[data.columns].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some histograms are less compressed while other remain the overall the same. This source of error might came from outliers in our dataset. We can easily check in some of the histograms above, for example ***n_tokens_title***, that the x-axis is extended for invisible values on the y-axis. Before discussing the most appropriate reaction, let's examine the values that have [1 in 10^50](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule) chance to exceed the mean, that is to say, the $D$ values such as $|D-\\mu_{feature}| > 15\\cdot \\sigma_{feature}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_outliers = ['n_tokens_content', 'num_hrefs', 'num_self_hrefs', 'num_imgs', 'num_videos', \n",
    "                      'kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', 'kw_max_max', 'kw_max_avg', 'kw_avg_avg', \n",
    "                      'self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_shares']\n",
    "\n",
    "for column in potential_outliers :\n",
    "    mean = data[column].mean()\n",
    "    sigma = data[column].std()\n",
    "    D = list()\n",
    "    for d in data[column] :\n",
    "        if abs(d - mean) > 15 * sigma :\n",
    "            D.append(d)\n",
    "    print(column, \"mean =\", round(mean, 2), \"number of outliers =\", len(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier criteria is intrinsically related to its standard deviation. Thus, we won't consider the features who are already in the $[-1, 1]$ interval but only those whose histograms are compressed to surprising extreme values. After that, we'll check that the number of outliers in ***shares*** hasn't significatively change, this would mean that the extreme values were not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_indexes(column, degree) :\n",
    "    mean = data[column].mean()\n",
    "    sigma = data[column].std()\n",
    "    indexes = list()\n",
    "    for i in data.index :\n",
    "        if abs(data[column][i] - mean) > degree * sigma :\n",
    "            indexes.append(i)\n",
    "    return indexes\n",
    "\n",
    "data = data.drop(outliers_indexes('n_tokens_content', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('num_hrefs', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('num_self_hrefs', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('num_imgs', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('num_videos', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('kw_min_min', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('kw_max_min', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('kw_avg_min', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('kw_min_max', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('kw_max_max', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('kw_max_avg', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('kw_avg_avg', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('self_reference_min_shares', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('self_reference_max_shares', 15), axis = 0)\n",
    "data = data.drop(outliers_indexes('self_reference_avg_shares', 15), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot once more the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 20,25\n",
    "fig = data['num_hrefs'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undeniably, we have an easier time to estimate the different distributions, for example ***num_hrefs*** appears to be following an exponential distribution whereas in the precedent histogram it looked like one range of values dominated the rest outside that range. Let's check if we've significantly changed the explained variable after all these changes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.shares.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"OnlineNewsPopularity.csv\")[' shares'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the original dataset doesn't really differ to the transformed one. We've lost 3% of the dataset and 5% of standard deviation which was likely the result of noise as we've explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the dataset <a class = \"anchor\" id = \"section23\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic methods for visualization and transformation <a class = \"anchor\" id = \"section230\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the different methods that enable us to plot histograms, density plots, probability-probability plots as well as to transform data as we please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# examine() plots a column's density plot as well as prints out the skewness and kurtosis of its curve\n",
    "def examine(column) :\n",
    "    sns.distplot(data[column])\n",
    "    print('Skewness of the curve :', data[column].skew())\n",
    "    print('Kurtosis of the curve :', data[column].kurt())\n",
    "\n",
    "# transform() plots the before and after transformation of a column according to a specified model.\n",
    "# The different models correspond to the inverse of the law the column is supposed to follow so as to normalize it.\n",
    "# \"Normal\" standardizes the column, \"Log\" return the logarithm of the column, \"Log+1\" and \"Log+2\" also but prevent log(0) values.\n",
    "# (a, b) returns the inverse of the Beta distribution whose parameters are alpha = a and beta = b.\n",
    "def transform(column, method = \"Normal\") :\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.distplot(data[column], fit = norm);\n",
    "    (mu, sigma) = norm.fit(data[column])\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc = 1)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(str(column) + ' distribution')\n",
    "    plt.subplot(1,2,2)\n",
    "    res = stats.probplot(data[column], plot = plt)\n",
    "    plt.suptitle('Before transformation')\n",
    "\n",
    "    if method == \"Normal\" :\n",
    "        data[column] = (data[column] - data[column].mean()) / data[column].std()\n",
    "    elif method == \"Log\" :\n",
    "        data[column] = np.log(data[column])\n",
    "    elif method == \"Log+1\" :\n",
    "        data[column] = np.log1p(data[column])\n",
    "    elif method == \"Log+2\" :\n",
    "        data[column] = np.log(data[column] + 2)\n",
    "    else :\n",
    "        data[column] = stats.beta.pdf(data[column], a = method[0], b = method[1])\n",
    "        data[column] = data[column] / data[column].max()\n",
    "\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.distplot(data[column], fit = norm);\n",
    "    (mu, sigma) = norm.fit(data[column])\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc=1)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(str(column) + ' distribution')\n",
    "    plt.subplot(1,2,2)\n",
    "    res = stats.probplot(data[column], plot = plt)\n",
    "    plt.suptitle('After ' + str(method) + ' transformation')\n",
    "\n",
    "# hist() returns the histogram of one or more columns\n",
    "def hist(columns) :\n",
    "    plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "    fig = data[columns].hist()\n",
    "\n",
    "# corrplot() returns the corrplots of two or more columns based on the Pearson's or Spearman's correlation coefficient.\n",
    "def corrplot(columns, method = 'pearson') :\n",
    "    sns.heatmap(data[columns].corr(method),\n",
    "                vmin = -1, vmax = 1, cmap = 'coolwarm',\n",
    "                annot = True, square = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the target ***shares*** <a class = \"anchor\" id = \"section231\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the dependent variable ***shares***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['shares'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target ***shares*** is expressed in the hundreds with some exceptions that are in the hundred thousands. However, most of the features' values are contained in the $[-1, 1]$ interval. To avoid overfitting and underfitting certain features, we'll transform all features whose values surpass the range of the $[-1, 1]$ domain. As for the target variable, we'll keep in mind the possibility to scale the values but won't at the moment, as it is rarely significant to do so (memory overflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the feature ***n_tokens_title*** <a class = \"anchor\" id = \"section232\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the feature ***n_tokens_title***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine('n_tokens_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems fair to assume the probability distribution of ***n_tokens_title*** follows a normal law. Let's standardize it to follow $\\mathcal{N}(0, 1)$ law in order to restrain it as much as possible to the $[-1, 1]$ interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('n_tokens_title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the feature ***n_tokens_content*** <a class = \"anchor\" id = \"section233\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the feature ***n_tokens_content***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine('n_tokens_content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a probability distribution whose skewness, or asymmetry of the curve, and kurtosis, or tail of the curve, remind of an exponential law. By applying the exponential inverse function to our target ***shares***, the probability distribution shall be normalized. In turn, coefficients regulation methods such as lasso or ridge have an easier time dealing with this variable. Finally, through documentation the use of the **np.log1p** method which transforms the variable through the function $log(1+x)$ rather than the conventionnal **np.log** $log(x)$ method seemed more satisfying and prevented the undefined $log(0)$ value error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('n_tokens_content', \"Log+1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the feature ***n_non_stop_words*** <a class = \"anchor\" id = \"section234\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the feature ***n_non_stop_words***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine('n_non_stop_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature ***n_non_stop_words*** follows what appears to be an inverse exponential law. The results when normalizing through **np.log1m** and **$\\frac{X-\\mu}{\\sigma}$** were unsatisfying. We came to the conclusion that the distribution follows the Beta $\\beta(\\alpha,\\beta)$ distribution whose parameters we shall estimate via la méthode des moments. Indeed, if $X\\hookrightarrow\\beta(\\alpha,\\beta)$, then $E(X)=\\frac{\\alpha}{\\alpha+\\beta}$ and $V(X)=\\frac{\\alpha\\cdot\\beta}{(\\alpha+\\beta)^2\\cdot(\\alpha+\\beta+1)}$. If we estimate $E(X)$ and $V(X)$ by applying respectively **mean()** and **std()** to ***n_non_stop_words***, then we have $\\alpha = k\\cdot\\beta$ and $\\beta^2 + \\frac{1}{k+1}\\beta - \\frac{k}{(k+1)^3\\cdot V(X)}=0$ where $k=\\frac{E(X)}{1-E(X)}$. Let's first determine the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = data['n_non_stop_words'].mean()\n",
    "var = data['n_non_stop_words'].std() ** 2\n",
    "\n",
    "k = mean / (1 - mean)\n",
    "delta = (1/(k+1))**2 - 4 * 1 * (- k / ((k+1)**3 * var)) # delta > 0\n",
    "b1 = (-1/(k+1) + np.sqrt(delta)) / (2 * 1) # b1 > 0, possible solution\n",
    "b2 = (-1/(k+1) - np.sqrt(delta)) / (2 * 1) # b2 < 0, impossible solution\n",
    "\n",
    "beta = b1\n",
    "alpha = k * beta\n",
    "(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotted on a [casio](https://keisan.casio.com/exec/system/1180573226) the distribution fits. Let's now apply the beta density function **beta.pdf()** and the **max** scaling $x_{scale} = \\frac{x}{max(x)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('n_non_stop_words', (alpha, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability has been scaled in a shape that we hope neither affects the model (overfitting/underfitting) nor the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the feature ***num_hrefs*** <a class = \"anchor\" id = \"section235\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the feature ***num_hrefs***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine('num_hrefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature ***num_hrefs*** follows what appears to be an exponential law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('num_hrefs', \"Log+1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the feature ***num_self_hrefs*** <a class = \"anchor\" id = \"section236\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the feature ***num_self_hrefs***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine('num_self_hrefs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature ***num_self_hrefs*** follows what appears to be an exponential law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('num_self_hrefs', \"Log+1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the feature ***num_imgs*** <a class = \"anchor\" id = \"section237\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the feature ***num_imgs***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine('num_imgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature ***num_imgs*** follows what appears to be an exponential law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('num_imgs', \"Log+1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the feature ***num_videos*** <a class = \"anchor\" id = \"section238\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the feature ***num_videos***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine('num_videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature ***num_videos*** follows what appears to be an exponential law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('num_videos', \"Log+1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***rate_positive_words*** and ***rate_negative_words*** <a class = \"anchor\" id = \"section239\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the pair of features ***rate_positive_words*** and ***rate_negative_words***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['rate_positive_words', 'rate_negative_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['rate_positive_words', 'rate_negative_words'], 'spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through both Pearson's and Spearman's correlation coefficient methods, the linear dependancy between ***rate_positive_words*** and ***rate_negative_words*** is undeniable. To prevent the model to overfit by adjusting the weight he gives to both features, we shall remove the latter one. Indeed, if we know the value of the first feature we know exactly the value of the other one, the explained variance from both features is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('rate_negative_words', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***keywords*** <a class = \"anchor\" id = \"section2310\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the features ***kw_avg_avg***, ***kw_avg_max***, ***kw_avg_min***, ***kw_max_avg***, ***kw_max_max***, ***kw_max_min***, ***kw_min_avg***, ***kw_min_max*** and ***kw_min_min***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', \n",
    "          'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['kw_min_min', 'kw_max_min', 'kw_avg_min', 'kw_min_max', \n",
    "          'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'kw_avg_avg'], 'spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reasons as before, we'll remove the ***kw_avg_...*** features, as they are intrinsically defined by ***kw_min_...*** and ***kw_max_...***. Moreover, the average is likely estimated from the arithmetic mean $m = \\frac{a+b}{2}$. On the contrary to the geometric mean $m = \\sqrt{a\\cdot b}$, the arithmetic mean is heavily penalized by extreme values, thus the weights of ***kw_min_...*** and ***kw_max_...*** on the average is significant, as showcased by the linear correlation plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['kw_avg_min', 'kw_avg_max', 'kw_avg_avg'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(['kw_min_min', 'kw_max_min', 'kw_min_max', 'kw_max_max', 'kw_min_avg', 'kw_max_avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply **np.log** to all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('kw_min_min', 'Log+2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('kw_max_min', 'Log+1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('kw_min_max', 'Log+1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('kw_max_max', 'Log+1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('kw_min_avg', 'Log+2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('kw_max_avg', 'Log+1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the P-P plots are disfigured. This can be the consequence of a poor variable defintion or the persistence of outliers but only applied to the ***keywords*** attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***self_reference*** <a class = \"anchor\" id = \"section2311\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the features ***self_reference_min_shares***, ***self_reference_max_shares*** and ***self_reference_avg_shares***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_shares'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['self_reference_min_shares', 'self_reference_max_shares', 'self_reference_avg_shares'], 'spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reasons as before, we'll remove the ***self_reference_avg_shares*** feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('self_reference_avg_shares', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(['self_reference_min_shares', 'self_reference_max_shares'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('self_reference_min_shares', 'Log+1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform('self_reference_max_shares', 'Log+1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***data_channel*** <a class = \"anchor\" id = \"section2312\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the ***data_channel*** features are not all present. Indeed, we can see that some articles are classified under none of the categories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['data_channel_is_lifestyle'] + data['data_channel_is_entertainment'] + data['data_channel_is_bus'] +\n",
    "       data['data_channel_is_socmed'] + data['data_channel_is_tech'] + data['data_channel_is_world'] != 1][[\n",
    "    'data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', \n",
    "    'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus',\n",
    "       'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidently, all features are negatively correlated to each other, but there are no real $y = -x$ relations. We shall restrain ourselves to transform these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***weekday*** and ***is_weekend*** <a class = \"anchor\" id = \"section2313\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = data.boxplot(column=['shares'], by=['weekday_is_sunday', 'weekday_is_saturday', 'weekday_is_friday',\n",
    "                                         'weekday_is_thursday', 'weekday_is_wednesday', 'weekday_is_tuesday',\n",
    "                                         'weekday_is_monday'], return_type = None, grid = False, rot = 45, fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put a log10 scale on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalog10 = data.copy()\n",
    "datalog10.shares = np.log1p(datalog10.shares)\n",
    "datalog10.boxplot(column=['shares'], by=['weekday_is_sunday', 'weekday_is_saturday', 'weekday_is_friday',\n",
    "                                         'weekday_is_thursday', 'weekday_is_wednesday', 'weekday_is_tuesday',\n",
    "                                         'weekday_is_monday'], return_type = None, grid = False, rot = 45, fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tendancy to share during the end of the week (Friday to Sunday) is higher than the rest of the week. Moreover, the standard deviation of monday and wednesday is higher than tuesday and thursday. This all correlates to what we would expect in actuality. Evidently, ***weekday_is_sunday*** is correlated to the remaining ***weekday*** features. ***is_weekend*** is also correlated to both ***weekday_is_saturday*** and ***weekday_is_sunday***. For equivalent reasons to the ***rate_positive_words*** / ***rate_negative_words*** case, we'll omit these two features of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['weekday_is_sunday', 'is_weekend'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***LDA*** <a class = \"anchor\" id = \"section2314\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Latent Dirichlet allocation (LDA) is a statistical method that allows the \"gentrification\" of the individuals following different topics. Here we suppose that every article is described by its \"allegiance\" to 5 different topics. These topics are mathematically established. A more known example in the academic field would be the Principal Component Analysis, which constructs artificial variables that suffice to explain the dataset, most of the time, the PCA reveals that most of the variance is explained by fewer number of variables than there are. As it is a probability, the law of total probability applies. Thus, we expect $LDA_{topic \\: 0} = 1 - \\sum^{4}_{k=1} LDA_{topic \\: k}$. Let's examine the set of features closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't enough evidence to claim that two topics are related. It isn't a surprise since the LDA method constructs topics that are independent. Nevertheless, we'll remove ***LDA_00*** as it's a linear combination of the rest :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LDA'] = data['LDA_01'] + data['LDA_02'] + data['LDA_03'] + data['LDA_04'] \n",
    "corrplot(['LDA', 'LDA_00'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['LDA_00', 'LDA'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***global*** <a class = \"anchor\" id = \"section2315\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['global_subjectivity', 'global_sentiment_polarity', 'global_rate_positive_words', 'global_rate_negative_words'], 'spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***global_sentiment_polarity*** is correlated to ***global_rate_positive_words*** and ***global_rate_negative_words***. So, we decide to remove the former and keep the two latters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('global_sentiment_polarity', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***title*** <a class = \"anchor\" id = \"section2316\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['title_subjectivity', 'title_sentiment_polarity', 'abs_title_subjectivity', 'abs_title_sentiment_polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same reasons, we remove the ***abs_title_subjectivity*** and ***abs_title_sentiment_polarity***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['abs_title_subjectivity', 'abs_title_sentiment_polarity'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the features ***polarity*** <a class = \"anchor\" id = \"section2317\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrplot(['avg_positive_polarity', 'min_positive_polarity', 'max_positive_polarity',\n",
    "       'avg_negative_polarity', 'min_negative_polarity', 'max_negative_polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove ***avg_positive_polarity*** and ***avg_negative_polarity***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['avg_positive_polarity', 'avg_negative_polarity'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.regression.linear_model as sm \n",
    "import statsmodels.api as s\n",
    "data1 = data[['num_imgs', 'num_videos', 'n_tokens_title']]\n",
    "data1 = s.add_constant(data1)\n",
    "data2 = data[['num_imgs', 'num_videos', 'n_tokens_title', 'data_channel_is_lifestyle',\n",
    "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
    "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
    "       'data_channel_is_world', 'weekday_is_monday', 'weekday_is_tuesday',\n",
    "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
    "       'weekday_is_saturday']]\n",
    "data2 = s.add_constant(data2)\n",
    "ols1 = sm.OLS(exog = data1, endog = data['shares']).fit()\n",
    "#pred1= ols1.predict(\"rentrer le dataframe de l'utilisateur ici\")\n",
    "ols2 = sm.OLS(exog = data2, endog = data['shares']).fit()\n",
    "#pred2= ols2.predict(\"rentrer le dataframe de l'utilisateur ici\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg1 = LinearRegression().fit(data1, data['shares'])\n",
    "reg2 = LinearRegression().fit(data2, data['shares'])\n",
    "#pred1= reg1.predict(\"rentrer le dataframe de l'utilisateur ici\")\n",
    "#pred2= reg2.predict(\"rentrer le dataframe de l'utilisateur ici\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols1.summary2().tables[0][3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models <a class = \"anchor\" id = \"section3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem is of the **regression** type. We shall compare the following models : <br></br>\n",
    "**Forward selection with 10-fold cross-validation** on **transformed data** and **raw data** <br></br>\n",
    "**Random Forest** on **transformed data** and **raw data** <br></br>\n",
    "**Lasso regularization** on **transformed data** and **raw data** <br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics <a class = \"anchor\" id = \"section30\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use two metrics for our prediction error : <br></br><br></br>\n",
    "**Mean Absolute Error** : $MAE = \\frac{1}{n}\\sum^{n}_{k=1}|y_i - \\hat{y}_i|$ <br></br>\n",
    "**Root Mean Square Error** : $RMSE = \\sqrt{\\frac{1}{n}\\sum^{n}_{k=1}(y_i - \\hat{y}_i)^2}$. <br></br> <br></br>\n",
    "These metrics are expressed in the same units as the target ***shares***, their values should give more insight than other metrics such as the **Mean Square Error** as well as the **Mean Percentage Error**. <br></br>\n",
    "Moreover, the **MAE** gives the easier metric to interpret but is less sensitive to outliers as opposed to the **RMSE**. We don't know the requirements of the prediction problem, thus, for safety measures we'll only keep the **MAE** for insight but not for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(pred, real) :\n",
    "    return (pred - real).abs().mean()\n",
    "\n",
    "def RMSE(pred, real) :\n",
    "    return np.sqrt(((pred - real)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Forward selection with 10-fold cross-validation** <a class = \"anchor\" id = \"section31\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting for 10-fold cross-validation <a class = \"anchor\" id = \"section311\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **cross-validation** by spliting the dataset in 10 sets of same length. By default, the **cross-validation score** is the $R^2$ of the model. Nevertheless, our aim is to return the best predictions, not the most linear model, especially since we're in no position to confirm such hypothesis. We chose the **CV score** to be the **arithmetic mean** $\\frac{a_1+...+a_n}{n}$ where $a_k$ is the **RMSE** of the model where $set_k$ is the subset used of testing, while the rest is used for training. The **geometric mean** $\\sqrt[\\leftroot{-3}\\uproot{3}n]{a_1\\cdot...\\cdot a_n}$ is better at representing the average of a sample whose elements are not independent, which is obviously our case thanks to the cross validation, but the **arithmetic mean** is the most sensitive to outliers, which will be the most valuable information in our minimization of the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "set1 = list(); set2 = list(); set3 = list();\n",
    "set4 = list(); set5 = list(); set6 = list();\n",
    "set7 = list(); set8 = list(); set9 = list();\n",
    "set10 = list()\n",
    "sets = data.copy().index.tolist()\n",
    "\n",
    "while len(sets)//10 > 0 :\n",
    "    set1.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set2.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set3.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set4.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set5.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set6.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set7.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set8.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set9.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "    set10.append(sets.pop(random.randint(0, len(sets)-1)))\n",
    "set1 = set1 + sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression on transformed data <a class = \"anchor\" id = \"section312\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.regression.linear_model as sm \n",
    "import statsmodels.api as s\n",
    "import time\n",
    "features = data.drop('shares', axis = 1)\n",
    "current_features = []\n",
    "linearselection = pd.DataFrame(columns = ['feature_selected', 'rsquared_adj', 'rmse_train_score', 'rmse_test_score', 'mae_train_score', \n",
    "            'mae_test_score'])\n",
    "while len(features.columns) > 0 :\n",
    "    t = time.time()\n",
    "    for feature in features.columns :\n",
    "        optimum = (None, None)\n",
    "\n",
    "        set1train = data.loc[set2+set3+set4+set5+set6+set7+set8+set9+set10, current_features + [feature]]\n",
    "        set1train = s.add_constant(set1train)\n",
    "        set2train = data.loc[set1+set3+set4+set5+set6+set7+set8+set9+set10, current_features + [feature]]\n",
    "        set2train = s.add_constant(set2train)\n",
    "        set3train = data.loc[set2+set1+set4+set5+set6+set7+set8+set9+set10, current_features + [feature]]\n",
    "        set3train = s.add_constant(set3train)\n",
    "        set4train = data.loc[set2+set3+set1+set5+set6+set7+set8+set9+set10, current_features + [feature]]\n",
    "        set4train = s.add_constant(set4train)\n",
    "        set5train = data.loc[set2+set3+set4+set1+set6+set7+set8+set9+set10, current_features + [feature]]\n",
    "        set5train = s.add_constant(set5train)\n",
    "        set6train = data.loc[set2+set3+set4+set5+set1+set7+set8+set9+set10, current_features + [feature]]\n",
    "        set6train = s.add_constant(set6train)\n",
    "        set7train = data.loc[set2+set3+set4+set5+set6+set1+set8+set9+set10, current_features + [feature]]\n",
    "        set7train = s.add_constant(set7train)\n",
    "        set8train = data.loc[set2+set3+set4+set5+set6+set7+set1+set9+set10, current_features + [feature]]\n",
    "        set8train = s.add_constant(set8train)\n",
    "        set9train = data.loc[set2+set3+set4+set5+set6+set7+set8+set1+set10, current_features + [feature]]\n",
    "        set9train = s.add_constant(set9train)\n",
    "        set10train = data.loc[set2+set3+set4+set5+set6+set7+set8+set9+set1, current_features + [feature]]\n",
    "        set10train = s.add_constant(set10train)\n",
    "        \n",
    "        set1trainvalidation = data.loc[set2+set3+set4+set5+set6+set7+set8+set9+set10, 'shares']\n",
    "        set2trainvalidation = data.loc[set1+set3+set4+set5+set6+set7+set8+set9+set10, 'shares']\n",
    "        set3trainvalidation = data.loc[set2+set1+set4+set5+set6+set7+set8+set9+set10, 'shares']\n",
    "        set4trainvalidation = data.loc[set2+set3+set1+set5+set6+set7+set8+set9+set10, 'shares']\n",
    "        set5trainvalidation = data.loc[set2+set3+set4+set1+set6+set7+set8+set9+set10, 'shares']\n",
    "        set6trainvalidation = data.loc[set2+set3+set4+set5+set1+set7+set8+set9+set10, 'shares']\n",
    "        set7trainvalidation = data.loc[set2+set3+set4+set5+set6+set1+set8+set9+set10, 'shares']\n",
    "        set8trainvalidation = data.loc[set2+set3+set4+set5+set6+set7+set1+set9+set10, 'shares']\n",
    "        set9trainvalidation = data.loc[set2+set3+set4+set5+set6+set7+set8+set1+set10, 'shares']\n",
    "        set10trainvalidation = data.loc[set2+set3+set4+set5+set6+set7+set8+set9+set1, 'shares']\n",
    "\n",
    "        set1test = data.loc[set1, current_features + [feature]]\n",
    "        set1test = s.add_constant(set1test)\n",
    "        set2test = data.loc[set2, current_features + [feature]]\n",
    "        set2test = s.add_constant(set2test)\n",
    "        set3test = data.loc[set3, current_features + [feature]]\n",
    "        set3test = s.add_constant(set3test)\n",
    "        set4test = data.loc[set4, current_features + [feature]]\n",
    "        set4test = s.add_constant(set4test)\n",
    "        set5test = data.loc[set5, current_features + [feature]]\n",
    "        set5test = s.add_constant(set5test)\n",
    "        set6test = data.loc[set6, current_features + [feature]]\n",
    "        set6test = s.add_constant(set6test)\n",
    "        set7test = data.loc[set7, current_features + [feature]]\n",
    "        set7test = s.add_constant(set7test)\n",
    "        set8test = data.loc[set8, current_features + [feature]]\n",
    "        set8test = s.add_constant(set8test)\n",
    "        set9test = data.loc[set9, current_features + [feature]]\n",
    "        set9test = s.add_constant(set9test)\n",
    "        set10test = data.loc[set10, current_features + [feature]]\n",
    "        set10test = s.add_constant(set10test)\n",
    "\n",
    "        set1testvalidation = data.loc[set1, 'shares']\n",
    "        set2testvalidation = data.loc[set2, 'shares']\n",
    "        set3testvalidation = data.loc[set3, 'shares']\n",
    "        set4testvalidation = data.loc[set4, 'shares']\n",
    "        set5testvalidation = data.loc[set5, 'shares']\n",
    "        set6testvalidation = data.loc[set6, 'shares']\n",
    "        set7testvalidation = data.loc[set7, 'shares']\n",
    "        set8testvalidation = data.loc[set8, 'shares']\n",
    "        set9testvalidation = data.loc[set9, 'shares']\n",
    "        set10testvalidation = data.loc[set10, 'shares']\n",
    "\n",
    "        ols1 = sm.OLS(exog = set1train, endog = set1trainvalidation).fit()\n",
    "        ols1train = ols1.predict(set1train)\n",
    "        ols1test = ols1.predict(set1test)\n",
    "        ols2 = sm.OLS(exog = set2train, endog = set2trainvalidation).fit()\n",
    "        ols2train = ols2.predict(set2train)\n",
    "        ols2test = ols2.predict(set2test)\n",
    "        ols3 = sm.OLS(exog = set3train, endog = set3trainvalidation).fit()\n",
    "        ols3train = ols3.predict(set3train)\n",
    "        ols3test = ols3.predict(set3test)\n",
    "        ols4 = sm.OLS(exog = set4train, endog = set4trainvalidation).fit()\n",
    "        ols4train = ols4.predict(set4train)\n",
    "        ols4test = ols4.predict(set4test)\n",
    "        ols5 = sm.OLS(exog = set5train, endog = set5trainvalidation).fit()\n",
    "        ols5train = ols5.predict(set5train)\n",
    "        ols5test = ols5.predict(set5test)\n",
    "        ols6 = sm.OLS(exog = set6train, endog = set6trainvalidation).fit()\n",
    "        ols6train = ols6.predict(set6train)\n",
    "        ols6test = ols6.predict(set6test)\n",
    "        ols7 = sm.OLS(exog = set7train, endog = set7trainvalidation).fit()\n",
    "        ols7train = ols7.predict(set7train)\n",
    "        ols7test = ols7.predict(set7test)\n",
    "        ols8 = sm.OLS(exog = set8train, endog = set8trainvalidation).fit()\n",
    "        ols8train = ols8.predict(set8train)\n",
    "        ols8test = ols8.predict(set8test)\n",
    "        ols9 = sm.OLS(exog = set9train, endog = set9trainvalidation).fit()\n",
    "        ols9train = ols9.predict(set9train)\n",
    "        ols9test = ols9.predict(set9test)\n",
    "        ols10 = sm.OLS(exog = set10train, endog = set10trainvalidation).fit()\n",
    "        ols10train = ols10.predict(set10train)\n",
    "        ols10test = ols10.predict(set10test)\n",
    "\n",
    "        rmsetrain = 0\n",
    "        rmsetest = 0\n",
    "        maetrain = 0\n",
    "        maetest = 0\n",
    "        rsquared = 0\n",
    "\n",
    "        rmsetrain += (RMSE(ols1train, set1trainvalidation))\n",
    "        rmsetest += (RMSE(ols1test, set1testvalidation))\n",
    "        maetrain += (MAE(ols1train, set1trainvalidation))\n",
    "        maetest += (MAE(ols1test, set1testvalidation))\n",
    "        rsquared += float(ols1.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols2train, set2trainvalidation))\n",
    "        rmsetest += (RMSE(ols2test, set2testvalidation))\n",
    "        maetrain += (MAE(ols2train, set2trainvalidation))\n",
    "        maetest += (MAE(ols2test, set2testvalidation))\n",
    "        rsquared += float(ols2.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols3train, set3trainvalidation))\n",
    "        rmsetest += (RMSE(ols3test, set3testvalidation))\n",
    "        maetrain += (MAE(ols3train, set3trainvalidation))\n",
    "        maetest += (MAE(ols3test, set3testvalidation))\n",
    "        rsquared += float(ols3.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols4train, set4trainvalidation))\n",
    "        rmsetest += (RMSE(ols4test, set4testvalidation))\n",
    "        maetrain += (MAE(ols4train, set4trainvalidation))\n",
    "        maetest += (MAE(ols4test, set4testvalidation))\n",
    "        rsquared += float(ols4.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols5train, set5trainvalidation))\n",
    "        rmsetest += (RMSE(ols5test, set5testvalidation))\n",
    "        maetrain += (MAE(ols5train, set5trainvalidation))\n",
    "        maetest += (MAE(ols5test, set5testvalidation))\n",
    "        rsquared += float(ols5.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols6train, set6trainvalidation))\n",
    "        rmsetest += (RMSE(ols6test, set6testvalidation))\n",
    "        maetrain += (MAE(ols6train, set6trainvalidation))\n",
    "        maetest += (MAE(ols6test, set6testvalidation))\n",
    "        rsquared += float(ols6.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols7train, set7trainvalidation))\n",
    "        rmsetest += (RMSE(ols7test, set7testvalidation))\n",
    "        maetrain += (MAE(ols7train, set7trainvalidation))\n",
    "        maetest += (MAE(ols7test, set7testvalidation))\n",
    "        rsquared += float(ols7.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols8train, set8trainvalidation))\n",
    "        rmsetest += (RMSE(ols8test, set8testvalidation))\n",
    "        maetrain += (MAE(ols8train, set8trainvalidation))\n",
    "        maetest += (MAE(ols8test, set8testvalidation))\n",
    "        rsquared += float(ols8.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols9train, set9trainvalidation))\n",
    "        rmsetest += (RMSE(ols9test, set9testvalidation))\n",
    "        maetrain += (MAE(ols9train, set9trainvalidation))\n",
    "        maetest += (MAE(ols9test, set9testvalidation))\n",
    "        rsquared += float(ols9.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrain += (RMSE(ols10train, set10trainvalidation))\n",
    "        rmsetest += (RMSE(ols10test, set10testvalidation))\n",
    "        maetrain += (MAE(ols10train, set10trainvalidation))\n",
    "        maetest += (MAE(ols10test, set10testvalidation))\n",
    "        rsquared += float(ols10.summary2().tables[0][3][0])\n",
    "\n",
    "        rmsetrainscore = rmsetrain / 10\n",
    "        rmsetestscore = rmsetest / 10\n",
    "        maetrainscore = maetrain / 10\n",
    "        maetestscore = maetest / 10\n",
    "        rsquared = rsquared / 10\n",
    "        \n",
    "        if optimum[1] == None :\n",
    "            optimum = (feature, rmsetestscore)\n",
    "        elif optimum[1] > rmsetestscore :\n",
    "            optimum = (feature, rmsetestscore)\n",
    "\n",
    "    features = features.drop(optimum[0], 1)\n",
    "    current_features.append(optimum[0])\n",
    "    linearselection = linearselection.append({'feature_selected' : optimum[0], 'rsquared_adj' : rsquared, 'rmse_train_score' : rmsetrainscore, \n",
    "                                              'rmse_test_score' : rmsetestscore, 'mae_train_score' : maetrainscore,\n",
    "                                              'mae_test_score' : maetestscore}, ignore_index = True)\n",
    "    print(time.time() - t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linearselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(linearselection))], linearselection['rmse_train_score'], 'b')\n",
    "plt.plot([i for i in range(len(linearselection))], linearselection['rmse_test_score'], 'r')\n",
    "plt.show()\n",
    "plt.plot([i for i in range(len(linearselection))], linearselection['mae_train_score'], 'b')\n",
    "plt.plot([i for i in range(len(linearselection))], linearselection['mae_test_score'], 'r')\n",
    "plt.show()\n",
    "plt.plot([i for i in range(len(linearselection))], linearselection['rsquared_adj'], 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest** <a class = \"anchor\" id = \"section32\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "datatrain, datatest, sharestrain, sharestest = train_test_split(data.drop('shares', 1), data['shares'], train_size = 0.67)\n",
    "\n",
    "forestselection = pd.DataFrame(columns = ['trees', 'rmse_auto_train', 'rmse_auto_test', 'autoscore', 'rmse_sqrt_train', \n",
    "            'rmse_sqrt_test', 'sqrtscore', 'rmse_log_train', 'rmse_log_test', 'logscore'])\n",
    "\n",
    "for t in range(1, len(data.columns)) :\n",
    "    timer = time.time()\n",
    "    \n",
    "    treeauto = RandomForestRegressor(n_estimators = t, criterion = 'mse', max_features = 'auto')\n",
    "    treeauto.fit(datatrain, sharestrain)\n",
    "    rmseautotrain = RMSE(treeauto.predict(datatrain), sharestrain)\n",
    "    rmseautotest = RMSE(treeauto.predict(datatest), sharestest)\n",
    "    autoscore = treeauto.score(datatest, sharestest)\n",
    "    \n",
    "    treesqrt = RandomForestRegressor(n_estimators = t, criterion = 'mse', max_features = 'sqrt')\n",
    "    treesqrt.fit(datatrain, sharestrain)\n",
    "    rmsesqrttrain = RMSE(treesqrt.predict(datatrain), sharestrain)\n",
    "    rmsesqrttest = RMSE(treesqrt.predict(datatest), sharestest)\n",
    "    sqrtscore = treesqrt.score(datatest, sharestest)\n",
    "    \n",
    "    treelog = RandomForestRegressor(n_estimators = t, criterion = 'mse', max_features = 'log2')\n",
    "    treelog.fit(datatrain, sharestrain)\n",
    "    rmselogtrain = RMSE(treelog.predict(datatrain), sharestrain)\n",
    "    rmselogtest = RMSE(treelog.predict(datatest), sharestest)\n",
    "    logscore = treelog.score(datatest, sharestest)\n",
    "    \n",
    "    forestselection = forestselection.append({'trees' : t, 'rmse_auto_train' : rmseautotrain, 'rmse_auto_test' : rmseautotest, 'autoscore' : autoscore,\n",
    "                                             'rmse_sqrt_train' : rmsesqrttrain, 'rmse_sqrt_test' : rmsesqrttest, 'sqrtscore' : sqrtscore,\n",
    "                                             'rmse_log_train' : rmselogtrain, 'rmse_log_test' : rmselogtest, 'logscore' : logscore}, ignore_index = True)\n",
    "    \n",
    "    print(str(t), str(time.time() - timer))\n",
    "    \n",
    "forestselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(forestselection))], forestselection['rmse_auto_test'], 'b')\n",
    "plt.plot([i for i in range(len(forestselection))], forestselection['rmse_sqrt_test'], 'g')\n",
    "plt.plot([i for i in range(len(forestselection))], forestselection['rmse_log_test'], 'r')\n",
    "plt.show()\n",
    "plt.plot([i for i in range(len(forestselection))], forestselection['rmse_sqrt_train'], 'g')\n",
    "plt.plot([i for i in range(len(forestselection))], forestselection['rmse_log_train'], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_and_feature_imp = pd.DataFrame({'names': data.drop('shares', 1).columns,\n",
    "                                     'importance': treelog.feature_importances_})\n",
    "names_and_feature_imp.sort_values([\"importance\"], ascending=True, inplace=True)\n",
    "names_and_feature_imp.plot(kind='barh', title=\"Features' importances of the log tree\",figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LinearRegression** <a class = \"anchor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = LinearRegression()\n",
    "\n",
    "regr.fit(datatrain, sharestrain)\n",
    "\n",
    "s_pred = regr.predict(datatest)\n",
    "\n",
    "\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "print(\"Mean squared error:\", mean_squared_error(sharestest, s_pred))\n",
    "\n",
    "print('R2 :', r2_score(sharestest, s_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decision Tree** <a class = \"anchor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(datatrain,sharestrain)\n",
    "\n",
    "y_pred = regressor.predict(datatest)\n",
    "\n",
    "print(\"RMSE: \" + str(round(sqrt(mean_squared_error(sharestest,y_pred)),2)))\n",
    "print(\"R_squared: \" + str(round(r2_score(sharestest,y_pred),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask regression <a class = \"anchor\" id = \"section4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataapi =  data[['n_tokens_title', 'num_imgs', 'num_videos']]\n",
    "\n",
    "yy = data['shares']\n",
    "\n",
    "dataapi =  data[['n_tokens_title', 'num_imgs', 'num_videos']]\n",
    "\n",
    "yy = data['shares']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(dataapi, yy, train_size=0.8)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "regr = LinearRegression()\n",
    "\n",
    "model_saved1 = regr.fit(Xtrain, Ytrain)\n",
    "\n",
    "s_pred = regr.predict(Xtest)\n",
    "\n",
    "\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "\n",
    "print(\"Mean squared error:\", mean_squared_error(Ytest, s_pred))\n",
    "\n",
    "print('Variance score:', r2_score(Ytest, s_pred))\n",
    "\n",
    "!pip install joblib\n",
    "import joblib\n",
    "joblib.dump(model_saved1, \"./model_saved1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
